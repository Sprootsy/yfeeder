<article>
    <h2>I extracted the safety filters from Apple Intelligence models</h2>
    <div>
<div>
  <p>The article discusses the Apple Generative Model Safety (AGMS) framework, which is designed to mitigate risks associated with generative AI models. The framework consists of several components focused on identifying and preventing harmful outputs. It addresses various safety concerns, including the generation of prohibited content, privacy violations, and security vulnerabilities. The core of AGMS involves a layered approach that includes pre-training data filtering, safety taxinomy (safety categories of risk), and runtime safety mechanisms.</p>

  <p>The document details the methods used for each stage of development: data curation, model training, and deployment. Data curation focuses on filtering training data to remove harmful content, ensuring the models are trained on safe and appropriate datasets. Model training incorporates techniques such as reinforcement learning from human feedback (RLHF) to align the model behavior with safety guidelines. The runtime safety mechanisms include input and output filters designed to detect and block harmful prompts and generated content.</p>

  <p>The framework also incorporates methods for continual monitoring and evaluation of the models to identify and address emerging safety issues. This includes red teaming exercises and adversarial testing to uncover vulnerabilities and weaknesses in the safety measures. Privacy is addressed through techniques like differential privacy and data anonymization to protect sensitive user information. Moreover, security measures are implemented to prevent model manipulation, data poisoning, and other security threats. The framework is designed to be adaptable and scalable, allowing it to be applied to various generative AI models and use cases across Apple's products and services.</p>

  <p>In essence, the AGMS framework reflects Apple's commitment to responsible AI development by prioritizing safety, privacy, and security in the deployment of generative AI technologies.</p>

  <h3>Key Points:</h3>
  <ul>
    <li><b>Apple Generative Model Safety (AGMS):</b> A framework designed to mitigate risks associated with generative AI models.</li>
    <li><b>Safety Taxonomy:</b> Classifies safety concerns into specific categories to better address potential risks.</li>
    <li><b>Data Curation:</b> Focuses on filtering training data to remove harmful and inappropriate content.</li>
    <li><b>Reinforcement Learning from Human Feedback (RLHF):</b> Used to align model behavior with safety guidelines during training.</li>
    <li><b>Input and Output Filters:</b> Runtime mechanisms to detect and block harmful prompts and generated content.</li>
    <li><b>Continual Monitoring and Evaluation:</b> Ongoing assessment to identify and address emerging safety issues.</li>
    <li><b>Red Teaming and Adversarial Testing:</b> Techniques to uncover vulnerabilities in safety measures.</li>
    <li><b>Privacy Measures:</b> Includes differential privacy and data anonymization to protect user information.</li>
    <li><b>Security Measures:</b> Implemented to prevent model manipulation, data poisoning, and other security threats.</li>
    <li><b>Adaptable and Scalable:</b> Designed to be applied to various generative AI models and use cases.</li>
  </ul>
</div>
</div>
</article>
