<article>
    <h2>EU rules ask tech giants to publicly track how, when AI models go off the rails</h2>
    <div>
 <div>
  <p>The European Union's AI Act is poised to become a landmark regulation, establishing a comprehensive legal framework for artificial intelligence. The law classifies AI systems based on risk, with the most dangerous applications facing outright bans. These include AI systems that use subliminal techniques, exploit vulnerabilities, or employ real-time biometric identification in public spaces (with limited exceptions). High-risk AI systems, which include those used in critical infrastructure, education, employment, essential private and public services (healthcare, banking), law enforcement, border control, and justice administration, will be subject to strict requirements before they can be deployed. These requirements include risk assessment, high-quality data usage, transparency, human oversight, and accuracy. Providers of these systems will need to demonstrate compliance before placing their products on the market and will face ongoing monitoring.</p>
  <p>AI systems deemed to pose limited risk will face lighter obligations, mainly focused on transparency, such as informing users when they are interacting with an AI. Certain general-purpose AI models will also be subject to specific transparency rules. The enforcement of the AI Act will be handled at the national level, with a European AI Office established to oversee implementation and coordinate standards. Companies that violate the rules could face fines of up to 7% of their global turnover, or €35 million, whichever is higher. The AI Act's broad scope and stringent requirements are expected to significantly impact the development and deployment of AI systems within the EU, potentially setting a global standard for AI regulation. The law is likely to be fully applicable 24 months after it comes into force, although some provisions will take effect sooner.</p>
  <p><strong>Key Points:</strong></p>
  <ul>
   <li>The EU AI Act categorizes AI systems based on risk, banning the highest-risk applications.</li>
   <li>High-risk AI systems (e.g., in critical infrastructure, healthcare, law enforcement) face strict requirements: risk assessment, data quality, transparency, human oversight, and accuracy.</li>
   <li>Limited-risk AI systems have lighter transparency obligations.</li>
   <li>General-purpose AI models are subject to specific transparency rules.</li>
   <li>Enforcement is at the national level, overseen by a European AI Office.</li>
   <li>Violations can result in fines up to 7% of global turnover or €35 million.</li>
   <li>The law will be fully applicable 24 months after it comes into force, with some provisions earlier.</li>
  </ul>
 </div>
 </div>
</article>
