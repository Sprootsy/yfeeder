<article>
    <h2>The new skill in AI is not prompting, it&#39;s context engineering</h2>
    <div>
<div>
<h2>Summary</h2>
<p>The article "Context Engineering Guide: LLMs, Prompt Engineering &amp; RAG" by Philipp Schmid discusses context engineering, a crucial aspect of working with Large Language Models (LLMs). It emphasizes that while prompt engineering focuses on crafting effective prompts, context engineering encompasses a broader approach of providing LLMs with the necessary information to generate accurate and relevant responses. The article highlights the limitations of LLMs, such as their fixed knowledge and context windows, and introduces techniques to overcome these limitations.</p>

<p>The article begins by outlining the differences between Prompt Engineering and Context Engineering, explaining that Context Engineering is an extension of Prompt Engineering. It explores the challenges of LLMs, including hallucination (generating incorrect or nonsensical information), limited knowledge, and context window constraints. It then delves into Retrieval Augmented Generation (RAG) as a primary method for Context Engineering, detailing its components: Indexing, Retrieval, and Generation. The indexing process involves loading, transforming, chunking, and embedding the data to create a vector database. Retrieval focuses on efficiently searching the vector database for relevant context based on a user query. The generation stage involves using the retrieved context along with the prompt to generate a response from the LLM.</p>

<p>The author outlines various techniques for optimizing each stage of the RAG pipeline. During indexing, strategies like data cleaning, metadata extraction, chunking, and vector embedding are discussed. For retrieval, techniques such as query transformations, re-ranking, and routing are covered. For generation, prompt optimization is highlighted, emphasizing the importance of clear instructions and context integration. The article also touches upon advanced RAG strategies, including query expansion, multi-hop queries, and knowledge graph integration.</p>

<p>The article discusses the importance of evaluating the RAG pipeline and suggests methods such as using ground truth data, conducting human evaluations, and employing LLM-based evaluations. It further explores tools and frameworks that simplify the implementation and management of RAG pipelines, such as LangChain, LlamaIndex, and Haystack. These tools provide abstractions and components for building and deploying RAG applications more efficiently.</p>

<p>In conclusion, the article presents context engineering as a critical skill for leveraging the power of LLMs effectively. By providing LLMs with the right context through techniques like RAG, it's possible to mitigate their limitations and create more accurate, relevant, and reliable AI-powered applications. The article serves as a comprehensive guide for practitioners looking to implement and optimize context engineering strategies in their projects.</p>

<h2>Key Points</h2>
<ul>
  <li><b>Context Engineering vs. Prompt Engineering:</b> Context engineering is a broader approach than prompt engineering, focusing on providing LLMs with the necessary information.</li>
  <li><b>LLM Limitations:</b> LLMs have limitations such as fixed knowledge, limited context windows, and a tendency to hallucinate.</li>
  <li><b>Retrieval Augmented Generation (RAG):</b> RAG is a primary method for context engineering, involving indexing, retrieval, and generation.</li>
  <li><b>Indexing:</b>  The indexing process includes loading, transforming, chunking, and embedding data to create a vector database.</li>
  <li><b>Retrieval:</b> Efficiently searching the vector database based on user queries to find relevant context.</li>
  <li><b>Generation:</b> Using the retrieved context along with the prompt to generate a response from the LLM.</li>
  <li><b>Optimization Techniques:</b> The article outlines techniques for optimizing each stage of the RAG pipeline, including data cleaning, chunking strategies, query transformations, and prompt optimization.</li>
  <li><b>Advanced RAG Strategies:</b> Advanced techniques include query expansion, multi-hop queries, and knowledge graph integration.</li>
  <li><b>Evaluation:</b>  Evaluating the RAG pipeline using ground truth data, human evaluations, and LLM-based evaluations is crucial.</li>
  <li><b>Tools and Frameworks:</b> Tools like LangChain, LlamaIndex, and Haystack simplify the implementation and management of RAG pipelines.</li>
</ul>
</div>
</div>
</article>
