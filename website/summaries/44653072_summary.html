<article>
    <h2>Qwen3-Coder: Agentic coding in the world</h2>
    <div>
<div>
  <p>The article introduces Qwen3-Coder, a code-oriented language model series developed by Qwen, available in three sizes: 1.5B, 4B, and 7B parameters. These models are designed to excel in code-related tasks, building upon the foundation of the QwenLM series and undergoing specialized training to enhance their coding capabilities. The models are pre-trained on a substantial volume of code and code-related data, followed by instruction fine-tuning using a diverse dataset to improve their ability to follow instructions and generate high-quality code. The Qwen3-Coder models support a context length of 8K tokens, allowing them to handle larger codebases and more complex coding tasks.</p>

  <p>The blog post details the training process, which involves pre-training on a mixture of code and code-related data, followed by instruction fine-tuning. The dataset used for instruction fine-tuning is composed of a variety of tasks including code generation, code completion, code translation, bug fixing, and code explanation. Careful data selection and cleaning are emphasized as crucial steps in achieving strong performance. The article also mentions the use of techniques like supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF) to further refine the models' performance.</p>

  <p>The article presents a comprehensive evaluation of Qwen3-Coder across a wide range of coding benchmarks, including HumanEval, MBPP, DS-1000, and CodeContests. The results demonstrate that Qwen3-Coder models achieve state-of-the-art performance among open-source code models of similar sizes. Specifically, Qwen3-Coder-7B outperforms other open-source models in the 7B parameter range, showcasing its ability to generate accurate and efficient code. The evaluation covers various aspects of coding proficiency, such as code generation from natural language descriptions, code completion, and problem-solving in competitive programming scenarios. The article emphasizes the models' ability to handle diverse coding tasks and programming languages.</p>

  <p>The blog post also discusses practical applications of Qwen3-Coder, highlighting its potential to assist developers in various aspects of the software development lifecycle. These applications include code generation, debugging, code explanation, and automated code review. The models can be used to accelerate development workflows, improve code quality, and reduce the time and effort required for software development. The article also mentions the models' potential to democratize access to coding skills, enabling individuals with limited programming experience to generate and understand code more easily.</p>

  <p>The article provides access to the Qwen3-Coder models, making them available for research and development purposes. The models are released under an open-source license, allowing developers and researchers to freely use, modify, and distribute them. The authors encourage the community to explore the models' capabilities, contribute to their development, and use them to build innovative coding applications. The blog post also includes information on how to access the models via Hugging Face Transformers and ModelScope, providing detailed instructions and code examples to facilitate their usage.</p>

  <p>Finally, the article expresses the authors' hope that Qwen3-Coder will serve as a valuable tool for the coding community, fostering innovation and collaboration in the field of software development. They encourage users to provide feedback and contribute to the ongoing development of the Qwen series.</p>

  <h2>Key Points:</h2>
  <ul>
    <li>Qwen3-Coder is a code-oriented language model series with 1.5B, 4B, and 7B parameter versions.</li>
    <li>It's built upon the QwenLM series and fine-tuned specifically for coding tasks.</li>
    <li>Training involved pre-training on code and code-related data, followed by instruction fine-tuning with a diverse dataset.</li>
    <li>Qwen3-Coder models support a context length of 8K tokens.</li>
    <li>Models achieve state-of-the-art performance among open-source code models of similar sizes across benchmarks like HumanEval, MBPP, and CodeContests.</li>
    <li>Potential applications include code generation, debugging, code explanation, and automated code review.</li>
    <li>Models are released under an open-source license and are accessible via Hugging Face Transformers and ModelScope.</li>
  </ul>
</div>
</div>
</article>
